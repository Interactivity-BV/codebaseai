"""
This script is designed to interact with the OpenAI API using a chain of runnables to process input data and generate AI responses.
It utilizes environment variables for configuration and includes logging for error handling. The script is intended for use with
the OpenAI GPT-4 model and requires an API key to function.

Modules:
- langchain_core.output_parsers: Provides output parsers for processing AI responses.
- langchain_core.runnables: Contains runnable components for building processing chains.
- langchain_openai: Interfaces with OpenAI's language models.
- sys, os: Standard Python modules for system operations and environment management.
- logging: Standard Python module for logging error messages.
- dotenv: Loads environment variables from a .env file.

Classes:
- None

Functions:
- create_connection: Establishes a connection to the OpenAI API using the specified model.
- run_chain: Executes a chain of runnables to process input data and generate an AI response.
"""

from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_ollama.llms import OllamaLLM
from langchain_ollama import ChatOllama
import sys
import os
import logging
from dotenv import load_dotenv


# Create a logger object
logger = logging.getLogger(__name__)


def create_connection(model_name):
    """
    Establishes a connection to the OpenAI API using the specified model.

    Args:
        model_name (str): The name of the OpenAI model to use. Defaults to "gpt-4o".

    Returns:
        ChatOpenAI: An instance of the ChatOpenAI class configured with the specified model and API key.

    Raises:
        None

    Side Effects:
        None

    Future Work:
        - Consider allowing more configuration options for the connection.
    """
    return ChatOllama(temperature=0.1, model=model_name)


def run_chain(prompt, input_data, model_name, connection=None):
    """
    Executes a chain of runnables to process input data and generate an AI response.

    Args:
        prompt (ChatPromptTemplate): The prompt template to use for generating the AI response.
        input_data (str): The input data to be processed by the chain.
        model_name (str): The name of the Ollama model to use. Defaults to environment variable.
        connection (ChatOpenAI, optional): An existing connection to the OpenAI API. If not provided, a new connection is created.

    Returns:
        str: The response generated by the AI.

    Raises:
        Exception: If there is an error during the chain execution.

    Side Effects:
        - Logs an error message if the OpenAI API key is not found.
        - Exits the program if the API key is missing.

    Future Work:
        - Implement error handling for specific exceptions during chain execution.
        - Consider adding more detailed logging for debugging purposes.
    """
    response = ""
    try:
        model = OllamaLLM(model=model_name)

        chain = (
            {"input": RunnablePassthrough()}
            | prompt
            | model
            | StrOutputParser()
        )
        logger.debug(f"Input data to LLM: {prompt.format(input=input_data)}")
        response = chain.invoke(input_data)
        logger.debug(f"Response from LLM: {response}")
    except Exception as e:
        logger.error(f"Error during large language model execution: {e}")
        sys.exit(1)
    return response